{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img tyle=\"float: right;\"  src=\"http://minneanalytics.org/wp/wp-content/uploads/2018/04/BDT18_LP-02-02.jpg\" \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySPARK Simple Start with jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM open source pixiedust notebook spark kernel\n",
    "- [IBM PixieDust](https://github.com/ibm-watson-data-lab/pixiedust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:26:04.658037Z",
     "start_time": "2018-06-03T18:26:01.494594Z"
    }
   },
   "outputs": [],
   "source": [
    "import pixiedust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set spark progress monitor (not working yet with jupyter lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:26:04.672381Z",
     "start_time": "2018-06-03T18:26:04.663043Z"
    }
   },
   "outputs": [],
   "source": [
    "pixiedust.enableSparkJobProgressMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:26:04.773549Z",
     "start_time": "2018-06-03T18:26:04.679380Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:26:04.798790Z",
     "start_time": "2018-06-03T18:26:04.782161Z"
    }
   },
   "outputs": [],
   "source": [
    "csvPath = \"file:///media/sf_mnlytics/data/fake_customers_100.csv.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read csv file using PySpark API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:26:21.421262Z",
     "start_time": "2018-06-03T18:26:04.810651Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(csvPath,header=True,sep=\"\\t\",ignoreLeadingWhiteSpace=True,ignoreTrailingWhiteSpace=True,inferSchema=False,multiLine=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print spark dataframe schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:26:21.487767Z",
     "start_time": "2018-06-03T18:26:21.428259Z"
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature transformation - problem: zipcode was read as integer and must be fixed then converted to string\n",
    "- if null then \"99999'\n",
    "- if length >= 5 then use asis\n",
    "- if length < 5 pad with zeroes on the left.\n",
    "- drop current zipcode column\n",
    "- rename computed_0 column to zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:26:27.833912Z",
     "start_time": "2018-06-03T18:26:21.490669Z"
    }
   },
   "outputs": [],
   "source": [
    "transformation_0 = df.withColumn(\"computed_0\",\\\n",
    "                                (when(col(\"zipcode\").isNull(),lit(99999))\\\n",
    "                                .when(length(col(\"zipcode\")) >= 5,col('zipcode'))\\\n",
    "                                .when(length(col(\"zipcode\")) < 5, concat(expr(\"substring('00000',1,5-length(zipcode))\"),col(\"zipcode\")))\\\n",
    "                                .otherwise(col(\"zipcode\"))).cast(StringType()))\n",
    "\n",
    "transformation_1 = transformation_0\\\n",
    "                        .drop(\"zipcode\")\n",
    "\n",
    "transformation_2 = transformation_1\\\n",
    "                        .withColumnRenamed(\"computed_0\",\"zipcode\")\n",
    "\n",
    "transformation_2\\\n",
    "    .select(\"zipcode\")\\\n",
    "    .filter(col(\"zipcode\").startswith(\"0\"))\\\n",
    "    .show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use describe function on a columns subset based on their data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:26:37.930481Z",
     "start_time": "2018-06-03T18:26:27.847565Z"
    }
   },
   "outputs": [],
   "source": [
    "transformation_2\\\n",
    "    .describe(*[c for c, t in transformation_2.dtypes if t in [\"int\",\"double\",\"bigint\",\"long\"]])\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop a column then sort on zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:26:38.129538Z",
     "start_time": "2018-06-03T18:26:37.941168Z"
    }
   },
   "outputs": [],
   "source": [
    "transformation_3 = transformation_2\\\n",
    "    .drop(\"credit_card_full\")\\\n",
    "    .sort(asc(\"zipcode\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:26:41.934912Z",
     "start_time": "2018-06-03T18:26:38.138572Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformation_3\\\n",
    "    .show(n=10,truncate=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create in memory temporary \"SQL\" table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:26:43.090055Z",
     "start_time": "2018-06-03T18:26:41.944620Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = transformation_3.createTempView(\"customers\")\n",
    "sqlContext.tableNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query in memory table using spark \"SQL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:26:45.071342Z",
     "start_time": "2018-06-03T18:26:43.120342Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from customers\")\\\n",
    "    .show(n=10,truncate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop the in memory temporary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:26:45.276663Z",
     "start_time": "2018-06-03T18:26:45.084366Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = sqlContext.dropTempTable(\"customers\")\n",
    "sqlContext.tableNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### persist dataframe to hadoop file system using parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:27:30.309644Z",
     "start_time": "2018-06-03T18:26:45.289535Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture dummy\n",
    "_ = transformation_3\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .parquet(\"/data/mnlytics.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use command line hdfs utility to query hadoop file system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:27:34.078416Z",
     "start_time": "2018-06-03T18:27:30.372324Z"
    }
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read parquet file data within a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:27:39.586558Z",
     "start_time": "2018-06-03T18:27:34.090210Z"
    }
   },
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(\"/data/mnlytics.parquet\")\n",
    "df_parquet.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### python famous libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:27:39.639053Z",
     "start_time": "2018-06-03T18:27:39.594780Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io import sql\n",
    "import matplotlib.style\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set matplotlib styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:27:39.802694Z",
     "start_time": "2018-06-03T18:27:39.644385Z"
    }
   },
   "outputs": [],
   "source": [
    "matplotlib.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:27:39.838334Z",
     "start_time": "2018-06-03T18:27:39.810005Z"
    }
   },
   "outputs": [],
   "source": [
    "_ = mpl.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a pandas dataframe from a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:27:47.654608Z",
     "start_time": "2018-06-03T18:27:39.844976Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture dummy\n",
    "df_pandas = df_parquet.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupby then filter pandas dataframe finally plot the result\n",
    "- groupby job and count\n",
    "- filter job with count > 65 using lambda expression\n",
    "- dropna\n",
    "- plot using matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T18:27:48.832294Z",
     "start_time": "2018-06-03T18:27:47.659209Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pandas[[\"job\"]]\\\n",
    "    .groupby([\"job\"])\\\n",
    "    .size()\\\n",
    "    .where(lambda count : count > 65)\\\n",
    "    .dropna()\\\n",
    "    .plot(kind=\"barh\",figsize=(20,10),title=\"Top Jobs\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataframe to permanent hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.write.mode('overwrite').saveAsTable(\"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from customer\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"drop table customer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "snake27.spark",
   "language": "python",
   "name": "pythonwithpixiedustspark23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
